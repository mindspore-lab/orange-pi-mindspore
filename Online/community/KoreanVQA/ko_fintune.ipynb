{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºäº OrangePi çš„å¤šæ¨¡æ€éŸ©è¯­ VQA å¾®è°ƒå…¨æµç¨‹\n",
    "\n",
    "æœ¬ Notebook æ•´åˆäº†ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹å¯¼å‡ºçš„å®Œæ•´å¾®è°ƒæµç¨‹ã€‚åŸºäº OpenDataLab â€œä¸‡å·Â·ä¸è·¯â€ éŸ©è¯­æ•°æ®ï¼Œä½¿ç”¨ LLaMA-Factory å¯¹ Qwen2-VL æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒã€‚\n",
    "\n",
    "> **æ³¨æ„**ï¼šæœ¬æµç¨‹ä¸åŒ…å«æ¨ç†æµ‹è¯•éƒ¨åˆ†ï¼Œä¸“æ³¨äºæ¨¡å‹çš„è®­ç»ƒä¸å¯¼å‡ºã€‚\n",
    "\n",
    "## ğŸ“‹ æµç¨‹ç›®å½•\n",
    "1. **ç¯å¢ƒå‡†å¤‡**ï¼šä¾èµ–å®‰è£…ä¸è·¯å¾„é…ç½®\n",
    "2. **æ•°æ®å¤„ç†**ï¼š\n",
    "   - å›¾ç‰‡ä¸‹è½½ä¸ç´¢å¼•æ„å»º\n",
    "   - æŸåå›¾ç‰‡æ¸…æ´—\n",
    "   - ShareGPT æ ¼å¼è½¬æ¢\n",
    "   - è®­ç»ƒ/éªŒè¯é›†åˆ†å‰²\n",
    "3. **æ•°æ®é›†æ³¨å†Œ**ï¼šé…ç½® LLaMA-Factory\n",
    "4. **æ¨¡å‹å¾®è°ƒ**ï¼šå¯åŠ¨ LoRA è®­ç»ƒ\n",
    "5. **æ¨¡å‹å¯¼å‡º**ï¼šåˆå¹¶æƒé‡å¹¶å¯¼å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡ä¸é…ç½®\n",
    "\n",
    "é¦–å…ˆå®šä¹‰å·¥ç¨‹ç›®å½•ç»“æ„ï¼Œå¹¶å®‰è£…å¿…è¦çš„ä¾èµ–åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modelscope in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (1.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (4.65.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (from modelscope) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\14468\\.conda\\envs\\pytorch\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# === å®‰è£…ä¾èµ– ===\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!pip install -e LLaMA-Factory/[metrics]\n",
    "!pip install modelscope requests tqdm pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb480f",
   "metadata": {},
   "source": [
    "### éœ€ä¸‹è½½â€ä¹¦ç”Ÿä¸‡å·â€œéŸ©è¯­å›¾æ–‡æ ‡æ³¨æ•°æ®é›†å¹¶ä¸”æ”¾åˆ°å·¥ä½œç›®å½•ä¸­\n",
    "https://opendatalab.com/OpenDataLab/WanJuanSiLu2O/blob/main/raw/image/ko/ko_image_caption.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·¥ä½œç›®å½•å·²å°±ç»ª: e:\\Desktop\\workspace\\orange-pi-mindspore\\Online\\community\\KoreanVQA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import hashlib\n",
    "import argparse\n",
    "import requests\n",
    "import concurrent.futures as futures\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from PIL import Image\n",
    "# ä¿®æ”¹åçš„å†™æ³•ï¼ˆçº¯æ–‡æœ¬è¿›åº¦æ¡ï¼Œå…¼å®¹æ€§å¥½ï¼‰\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === å…¨å±€è·¯å¾„é…ç½® ===\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "IMAGE_ROOT = os.path.join(DATA_DIR, \"images\", \"ko\")\n",
    "\n",
    "# è¾“å…¥æ–‡ä»¶ (è¯·ç¡®ä¿æ­¤æ–‡ä»¶å·²ä¸Šä¼ åˆ°å·¥ä½œç›®å½•)\n",
    "RAW_DATA_PATH = \"ko_image_caption.jsonl\"\n",
    "\n",
    "# ä¸­é—´äº§ç‰©\n",
    "CLEAN_JSON_PATH = os.path.join(DATA_DIR, \"ko_caption_clean.json\")\n",
    "SHAREGPT_PATH = os.path.join(DATA_DIR, \"ko_sharegpt.json\")\n",
    "\n",
    "# æœ€ç»ˆæ•°æ®é›†\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"ko_train.json\")\n",
    "EVAL_FILE = os.path.join(DATA_DIR, \"ko_eval.json\")\n",
    "\n",
    "# æ¨¡å‹ç›¸å…³\n",
    "MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "OUTPUT_DIR = \"saves/Qwen2-VL/lora/Qwen2-VL-sft-ko\"\n",
    "EXPORT_DIR = \"models/Qwen2-VL-sft-final\"\n",
    "\n",
    "# åˆ›å»ºå¿…è¦ç›®å½•\n",
    "os.makedirs(IMAGE_ROOT, exist_ok=True)\n",
    "print(f\"å·¥ä½œç›®å½•å·²å°±ç»ª: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®å¤„ç†ï¼šå›¾ç‰‡ä¸‹è½½\n",
    "\n",
    "è¯»å–åŸå§‹ JSONLï¼Œå¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œè¿‡æ»¤æ‰ä¸å¯ä¸‹è½½çš„æ–‡ä»¶\n",
    "\n",
    "å¯é€‰æ‹©ä¸‹è½½çš„æ•°æ®é›†æ•°é‡ï¼Œæ›¿æ¢è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹å¤„ç†å›¾ç‰‡ï¼Œæºæ–‡ä»¶: ko_image_caption.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [16:15<00:00,  2.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸‹è½½å®Œæˆã€‚æˆåŠŸ: 635/2000ã€‚ç»“æœå·²ä¿å­˜è‡³: e:\\Desktop\\workspace\\orange-pi-mindspore\\Online\\community\\KoreanVQA\\data\\ko_caption_clean.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === é…ç½®ä¸‹è½½å‚æ•° ===\n",
    "MAX_DOWNLOAD_LINES = 2000  # é™åˆ¶å¤„ç†è¡Œæ•°ï¼Œ0ä¸ºå¤„ç†å…¨éƒ¨\n",
    "NUM_WORKERS = 32           # ä¸‹è½½å¹¶å‘æ•°\n",
    "\n",
    "# === æ ¸å¿ƒä¸‹è½½é€»è¾‘ ===\n",
    "FORMAT_EXT_MAP = {\n",
    "    \"JPEG\": \".jpg\", \"JPG\": \".jpg\", \"PNG\": \".png\",\n",
    "    \"WEBP\": \".webp\", \"BMP\": \".bmp\", \"GIF\": \".gif\",\n",
    "}\n",
    "\n",
    "def guess_ext(url: str, fmt: Optional[str]) -> str:\n",
    "    path = urlparse(url).path\n",
    "    suf = Path(path).suffix.lower()\n",
    "    if suf in {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".gif\"}:\n",
    "        return \".jpg\" if suf == \".jpeg\" else suf\n",
    "    if fmt:\n",
    "        ext = FORMAT_EXT_MAP.get(fmt.upper())\n",
    "        if ext: return ext\n",
    "    return \".jpg\"\n",
    "\n",
    "def build_local_path(out_dir: Path, img_id: Optional[str], url: str, fmt: Optional[str]) -> Path:\n",
    "    ext = guess_ext(url, fmt)\n",
    "    if img_id and isinstance(img_id, str) and len(img_id) >= 2:\n",
    "        key = img_id\n",
    "    else:\n",
    "        key = hashlib.sha1(url.encode(\"utf-8\")).hexdigest()\n",
    "    subdir = key[:2]  # ä½¿ç”¨å‰ä¸¤ä½å“ˆå¸Œåˆ†æ¡¶\n",
    "    return out_dir / subdir / f\"{key}{ext}\"\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=64, pool_maxsize=64)\n",
    "    sess.mount(\"http://\", adapter)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    return sess\n",
    "\n",
    "def process_line(line: str, out_dir: Path) -> Tuple[bool, Optional[str]]:\n",
    "    try:\n",
    "        rec = json.loads(line)\n",
    "    except: return False, None\n",
    "\n",
    "    image = rec.get(\"image\", {}) or {}\n",
    "    url = image.get(\"path\")\n",
    "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return False, None\n",
    "\n",
    "    local_path = build_local_path(out_dir, rec.get(\"img_id\"), url, image.get(\"format\"))\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”éç©ºï¼Œè·³è¿‡ä¸‹è½½\n",
    "    if not (local_path.exists() and local_path.stat().st_size > 0):\n",
    "        try:\n",
    "            session = make_session()\n",
    "            with session.get(url, stream=True, timeout=5) as resp:\n",
    "                resp.raise_for_status()\n",
    "                with open(local_path, \"wb\") as f:\n",
    "                    for chunk in resp.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "        except Exception:\n",
    "            if local_path.exists(): local_path.unlink()\n",
    "            return False, None\n",
    "\n",
    "    image[\"path\"] = str(local_path.as_posix())\n",
    "    rec[\"image\"] = image\n",
    "    return True, json.dumps(rec, ensure_ascii=False)\n",
    "\n",
    "# === æ‰§è¡Œä¸‹è½½ä»»åŠ¡ ===\n",
    "print(f\"å¼€å§‹å¤„ç†å›¾ç‰‡ï¼Œæºæ–‡ä»¶: {RAW_DATA_PATH}\")\n",
    "if not os.path.exists(RAW_DATA_PATH):\n",
    "    print(\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶ï¼Œè¯·ä¸Šä¼  ko_image_caption.jsonl\")\n",
    "else:\n",
    "    lines = []\n",
    "    with open(RAW_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        if MAX_DOWNLOAD_LINES > 0:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= MAX_DOWNLOAD_LINES: break\n",
    "                lines.append(line.strip())\n",
    "        else:\n",
    "            lines = [l.strip() for l in f]\n",
    "\n",
    "    success_cnt = 0\n",
    "    with open(CLEAN_JSON_PATH, \"w\", encoding=\"utf-8\") as fw:\n",
    "        with futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n",
    "            tasks = [ex.submit(process_line, line, Path(IMAGE_ROOT)) for line in lines]\n",
    "            for fut in tqdm(futures.as_completed(tasks), total=len(tasks), desc=\"Downloading\"):\n",
    "                ok, res = fut.result()\n",
    "                if ok and res:\n",
    "                    fw.write(res + \"\\n\")\n",
    "                    success_cnt += 1\n",
    "    \n",
    "    print(f\"âœ… ä¸‹è½½å®Œæˆã€‚æˆåŠŸ: {success_cnt}/{len(lines)}ã€‚ç»“æœå·²ä¿å­˜è‡³: {CLEAN_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®å¤„ç†ï¼šæ¸…æ´—ä¸æ ¼å¼è½¬æ¢\n",
    "\n",
    "æ¸…æ´—åå›¾å¹¶ä¸”å°†æ•°æ®è½¬æ¢ä¸ºshareGPTæ ¼å¼\n",
    "è¿™é‡Œå°† Prompt ç»Ÿä¸€è®¾ç½®ä¸ºéŸ©è¯­ï¼š**\"ì´ ì‚¬ì§„ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"** (è¯·æè¿°è¿™å¼ å›¾ç‰‡)ã€‚\n",
    "\n",
    "shareGPTæ ·ä¾‹ï¼š  \n",
    "\n",
    "       {\n",
    "         \"messages\": [\n",
    "           {\"role\": \"user\", \"content\": \"<image>ê·¸ë“¤ì€ ëˆ„êµ¬ì…ë‹ˆê¹Œ?\"},\n",
    "           {\"role\": \"assistant\", \"content\": \"ê·¸ë“¤ì€ ë°”ì´ì—ë¥¸ ë®Œí—¨ì˜ ì¼€ì¸ê³¼ ê³ ë ˆì¸ ì¹´ì…ë‹ˆë‹¤.\"},\n",
    "         ],\n",
    "         \"images\": [\"demo_data/1.jpg\"]\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ ¼å¼è½¬æ¢ä¸åå›¾æ¸…æ´—...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 635it [00:11, 56.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è½¬æ¢å®Œæˆã€‚æœ‰æ•ˆæ•°æ®: 570 æ¡ã€‚ä¿å­˜è‡³: e:\\Desktop\\workspace\\orange-pi-mindspore\\Online\\community\\KoreanVQA\\data\\ko_sharegpt.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def verify_image(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"å¼€å§‹æ ¼å¼è½¬æ¢ä¸åå›¾æ¸…æ´—...\")\n",
    "sharegpt_data = []\n",
    "valid_count = 0\n",
    "\n",
    "if os.path.exists(CLEAN_JSON_PATH):\n",
    "    with open(CLEAN_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Processing\"):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                img_path = data.get(\"image\", {}).get(\"path\")\n",
    "                caption = data.get(\"captions\", {}).get(\"content\")\n",
    "                \n",
    "                # æ ¡éªŒå›¾ç‰‡\n",
    "                if not img_path or not os.path.exists(img_path) or not verify_image(img_path):\n",
    "                    continue\n",
    "                \n",
    "                # æ„å»º ShareGPT æ ¼å¼ (éŸ©è¯­ Prompt)\n",
    "                entry = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": \"<image>ì´ ì‚¬ì§„ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}, \n",
    "                        {\"role\": \"assistant\", \"content\": caption}\n",
    "                    ],\n",
    "                    \"images\": [img_path]\n",
    "                }\n",
    "                sharegpt_data.append(entry)\n",
    "                valid_count += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    with open(SHAREGPT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sharegpt_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… è½¬æ¢å®Œæˆã€‚æœ‰æ•ˆæ•°æ®: {valid_count} æ¡ã€‚ä¿å­˜è‡³: {SHAREGPT_PATH}\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°ä¸‹è½½åçš„æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤ 2ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®å¤„ç†ï¼šæ•°æ®é›†åˆ†å‰²\n",
    "\n",
    "å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ä¸éªŒè¯é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åˆ†å‰²å®Œæˆã€‚\n",
      "   è®­ç»ƒé›†: 70 -> e:\\Desktop\\workspace\\orange-pi-mindspore\\Online\\community\\KoreanVQA\\data\\ko_train.json\n",
      "   éªŒè¯é›†: 500 -> e:\\Desktop\\workspace\\orange-pi-mindspore\\Online\\community\\KoreanVQA\\data\\ko_eval.json\n"
     ]
    }
   ],
   "source": [
    "N_EVAL = 70  # éªŒè¯é›†æ•°é‡\n",
    "SEED = 42\n",
    "\n",
    "if os.path.exists(SHAREGPT_PATH):\n",
    "    with open(SHAREGPT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_data = json.load(f)\n",
    "    \n",
    "    total = len(all_data)\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    # ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿ\n",
    "    if total <= N_EVAL:\n",
    "        n_eval = int(total * 0.1)\n",
    "    else:\n",
    "        n_eval = N_EVAL\n",
    "        \n",
    "    n_train = total - n_eval\n",
    "    train_data = all_data[:n_train]\n",
    "    eval_data = all_data[n_train:]\n",
    "    \n",
    "    with open(TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "    with open(EVAL_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(f\"âœ… åˆ†å‰²å®Œæˆã€‚\\n   è®­ç»ƒé›†: {len(train_data)} -> {TRAIN_FILE}\\n   éªŒè¯é›†: {len(eval_data)} -> {EVAL_FILE}\")\n",
    "else:\n",
    "    print(\"âŒ æ‰¾ä¸åˆ° ShareGPT æ•°æ®æ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¸‹è½½åº•åº§æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://www.modelscope.cn/Qwen/Qwen2-VL-2B-Instruct.git models/Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9c8ce",
   "metadata": {},
   "source": [
    "## 5. æ³¨å†Œæ•°æ®é›†\n",
    "\n",
    "å°†ç”Ÿæˆçš„ json æ–‡ä»¶è·¯å¾„å†™å…¥ LLaMA-Factory çš„ `dataset_info.json` ä¸­ï¼Œä»¥ä¾¿æ¡†æ¶è¯†åˆ«ã€‚**\n",
    "\n",
    "       \"ko_train\": {\n",
    "         \"path\": \"data/ko_train.json\",\n",
    "         \"type\": \"sharegpt_multi_modal\"\n",
    "       },\n",
    "       \"ko_val\": {\n",
    "         \"path\": \"data/ko_val.json\",\n",
    "         \"type\": \"sharegpt_multi_modal\"\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹å¾®è°ƒ (LoRA)\n",
    "\n",
    "ä½¿ç”¨ LLaMA-Factory CLI å¯åŠ¨è®­ç»ƒã€‚\n",
    "\n",
    "### å…³é”®å‚æ•°ç¤ºä¾‹ï¼š\n",
    "\n",
    "   | é€‰é¡¹ | å€¼ |\n",
    "   | ---- | -- |\n",
    "   | Model name  | Qwen2-VL-2B-Instruct |\n",
    "   | Model path  | models/Qwen2-VL-2B-Instruct |\n",
    "   | Finetune    | LoRA |\n",
    "   | Stage       | Supervised Fine-Tuning |\n",
    "   | Dataset     | ko_train |\n",
    "   | Max epochs  | 3 |\n",
    "   | Batch size  | 16 |\n",
    "   | Save steps  | 200 |\n",
    "   | lora_rank   | 64 |\n",
    "   | lora_alpha  | 128ï¼ˆä¸€èˆ¬æ˜¯rankçš„ä¸¤å€ï¼‰ |\n",
    "   | lora_dropout | 0.05ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ |\n",
    "   | Output dir  | saves/Qwen2-VL/lora/Qwen2-VL-sft-ko |\n",
    "\n",
    "### ç›‘æ§æ˜¾å­˜  \n",
    "       watch -n 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹å¯¼å‡º\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œå°† LoRA æƒé‡åˆå¹¶åˆ°åº•åº§æ¨¡å‹ä¸­ï¼Œä»¥ä¾¿åœ¨ OrangePi ä¸Šéƒ¨ç½²ã€‚\n",
    "\n",
    "åœ¨ WebUI **Expert** æ ‡ç­¾æ‰§è¡Œ  \n",
    "\n",
    "    Model path      = models/Qwen2-VL-2B-Instruct\n",
    "    Checkpoint path = saves/Qwen2-VL/lora/Qwen2-VL-sft-ko\n",
    "    Export path     = models/Qwen2-VL-sft-final\n",
    "\n",
    "ç‚¹å‡»â€œå¼€å§‹å¯¼å‡ºâ€ï¼Œå¾—åˆ°åˆå¹¶æƒé‡ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
