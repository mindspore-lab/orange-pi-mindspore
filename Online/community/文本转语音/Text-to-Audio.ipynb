{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebd851e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore版本: 2.6.0.dev20250323\n",
      "当前使用设备: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.io import wavfile\n",
    "from matplotlib import pyplot as plt\n",
    "from pypinyin import pinyin, Style\n",
    "from string import punctuation\n",
    "from mindspore import context\n",
    "import hifigan\n",
    "from text.symbols import symbols\n",
    "print(\"MindSpore版本:\", ms.__version__)\n",
    "device = context.get_context(\"device_target\")\n",
    "if not device:\n",
    "    has_gpu = ms.compatible.get_device_id() != -1\n",
    "    device = \"GPU\" if has_gpu else \"CPU\"\n",
    "    context.set_context(device_target=device)\n",
    "\n",
    "print(f\"当前使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ba0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    if not isinstance(lengths, ms.Tensor):\n",
    "        lengths = ms.Tensor(lengths, dtype=ms.int64)\n",
    "    elif lengths.dtype != ms.int64:\n",
    "        lengths = ops.cast(lengths, ms.int64)\n",
    "\n",
    "    batch_size = int(lengths.shape[0])\n",
    "    if max_len is None:\n",
    "        max_len = int(ops.max(lengths)[0].asnumpy().item())\n",
    "    else:\n",
    "        if isinstance(max_len, ms.Tensor):\n",
    "            max_len_np = max_len.asnumpy()\n",
    "            max_len = int(max_len_np.item() if hasattr(max_len_np, \"item\") else np.array(max_len_np).reshape(-1)[0])\n",
    "        else:\n",
    "            max_len = int(max_len)\n",
    "    max_len = max(max_len, 1)\n",
    "    start = ms.Tensor(0.0, dtype=ms.float32)\n",
    "    end = ms.Tensor(float(max_len - 1), dtype=ms.float32)\n",
    "    ids = ops.linspace(start, end, max_len).reshape(1, -1)\n",
    "    ids = ops.broadcast_to(ids, (batch_size, max_len))\n",
    "    ids = ops.cast(ids, ms.int64)\n",
    "\n",
    "    lengths_expand = ops.expand_dims(lengths, 1)\n",
    "    lengths_expand = ops.broadcast_to(lengths_expand, (batch_size, max_len))\n",
    "    mask = ids >= lengths_expand\n",
    "    return mask\n",
    "\n",
    "\n",
    "def pad(input_ele, mel_max_length=None):\n",
    "    if mel_max_length:\n",
    "        max_len = mel_max_length\n",
    "    else:\n",
    "        max_len = max([batch.shape[0] for batch in input_ele])\n",
    "    \n",
    "    out_list = []\n",
    "    for batch in input_ele:\n",
    "        seq_len = batch.shape[0]\n",
    "        pad_right = max(max_len - seq_len, 0)\n",
    "        \n",
    "        if len(batch.shape) == 1:\n",
    "            pad_width = (0, pad_right)\n",
    "        elif len(batch.shape) == 2:\n",
    "            pad_width = (0, pad_right, 0, 0)\n",
    "        else:\n",
    "            raise ValueError(f\"仅支持1D/2D张量，当前是{len(batch.shape)}维\")\n",
    "        one_batch_padded = ops.pad(\n",
    "            input_x=batch,\n",
    "            padding=pad_width,\n",
    "            mode='constant',\n",
    "        )\n",
    "        out_list.append(one_batch_padded)\n",
    "    out_padded = ops.stack(out_list)\n",
    "    return out_padded\n",
    "\n",
    "\n",
    "def read_lexicon(lex_path):\n",
    "    lexicon = {}\n",
    "    with open(lex_path) as f:\n",
    "        for line in f:\n",
    "            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n",
    "            word = temp[0]\n",
    "            phones = temp[1:]\n",
    "            if word.lower() not in lexicon:\n",
    "                lexicon[word.lower()] = phones\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def preprocess_mandarin(text, preprocess_config):\n",
    "    text = text.rstrip(punctuation)\n",
    "    lexicon = read_lexicon(preprocess_config[\"path\"][\"lexicon_path\"])\n",
    "    pinyins = [p[0] for p in pinyin(text, style=Style.TONE3, strict=False, neutral_tone_with_five=True)]\n",
    "    phones = []\n",
    "    for p in pinyins:\n",
    "        phones += lexicon[p] if p in lexicon else [\"sp\"]\n",
    "    phones = \"{\" + \" \".join(phones) + \"}\"\n",
    "    print(f\"原始文本: {text}\")\n",
    "    print(f\"音素序列: {phones}\")\n",
    "    from text import text_to_sequence\n",
    "    sequence = np.array(text_to_sequence(phones, preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]))\n",
    "    if len(sequence) == 0:\n",
    "        raise ValueError(f\"文本预处理后序列长度为0，请检查输入文本或词典：{text}\")\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) \n",
    "    if padding_idx is not None:\n",
    "        sinusoid_table[padding_idx] = 0.0\n",
    "    return ms.Tensor(sinusoid_table, dtype=ms.float32)\n",
    "\n",
    "def expand(values, durations):\n",
    "    out = list()\n",
    "    for value, d in zip(values, durations):\n",
    "        out += [value] * max(0, int(d))\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60220cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthRegulator(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(LengthRegulator, self).__init__()\n",
    "    def LR(self, x, duration, max_len):\n",
    "        output = []\n",
    "        mel_len = []\n",
    "        for batch, expand_target in zip(x, duration):\n",
    "            expanded = self.expand(batch, expand_target)\n",
    "            output.append(expanded)\n",
    "            mel_len.append(expanded.shape[0])\n",
    "\n",
    "        output = pad(output, max_len) if max_len else pad(output)\n",
    "        return output, ms.Tensor(mel_len, dtype=ms.int64)\n",
    "    def expand(self, batch, predicted):\n",
    "        out = []\n",
    "        for i, vec in enumerate(batch):\n",
    "            expand_size = int(predicted[i].asnumpy()) \n",
    "            expanded_vec = ops.broadcast_to(vec, (max(expand_size, 0), vec.shape[0]))\n",
    "            out.append(expanded_vec)\n",
    "        return ops.concat(out, axis=0)\n",
    "    def construct(self, x, duration, max_len):\n",
    "        output, mel_len = self.LR(x, duration, max_len)\n",
    "        return output, mel_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3846aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            has_bias=bias,\n",
    "            pad_mode='pad'\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VariancePredictor(nn.Cell):\n",
    "    def __init__(self, model_config):\n",
    "        super(VariancePredictor, self).__init__()\n",
    "        self.input_size = model_config[\"transformer\"][\"encoder_hidden\"]\n",
    "        self.filter_size = model_config[\"variance_predictor\"][\"filter_size\"]\n",
    "        self.kernel = model_config[\"variance_predictor\"][\"kernel_size\"]\n",
    "        self.dropout = model_config[\"variance_predictor\"][\"dropout\"]\n",
    "        self.conv_layer = nn.SequentialCell(OrderedDict([\n",
    "            (\"conv1d_1\", Conv(\n",
    "                self.input_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=(self.kernel - 1) // 2,bias=True\n",
    "            )),\n",
    "            (\"relu_1\", nn.ReLU()),\n",
    "            (\"layer_norm_1\", nn.LayerNorm((self.filter_size,))),\n",
    "            (\"dropout_1\", nn.Dropout(p=self.dropout)),\n",
    "            (\"conv1d_2\", Conv(\n",
    "                self.filter_size, self.filter_size,\n",
    "                kernel_size=self.kernel, padding=1,bias=True\n",
    "            )),\n",
    "            (\"relu_2\", nn.ReLU()),\n",
    "            (\"layer_norm_2\", nn.LayerNorm((self.filter_size,))),\n",
    "            (\"dropout_2\", nn.Dropout(p=self.dropout))\n",
    "        ]))\n",
    "\n",
    "        self.linear_layer = nn.Dense(self.filter_size, 1)\n",
    "\n",
    "    def construct(self, encoder_output, mask):\n",
    "        out = self.conv_layer(encoder_output)\n",
    "        out = self.linear_layer(out)\n",
    "        out = out.squeeze(-1)\n",
    "\n",
    "        if mask is not None:\n",
    "            out = ops.masked_fill(out, mask.astype(ms.bool_), 0.0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceAdaptor(nn.Cell):\n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(VarianceAdaptor, self).__init__()\n",
    "        self.duration_predictor = VariancePredictor(model_config)\n",
    "        self.length_regulator = LengthRegulator()\n",
    "        self.pitch_predictor = VariancePredictor(model_config)\n",
    "        self.energy_predictor = VariancePredictor(model_config)\n",
    "        self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"]\n",
    "        self.energy_feature_level = preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"]\n",
    "        assert self.pitch_feature_level in [\"phoneme_level\", \"frame_level\"]\n",
    "        assert self.energy_feature_level in [\"phoneme_level\", \"frame_level\"]\n",
    "        pitch_quantization = model_config[\"variance_embedding\"][\"pitch_quantization\"]\n",
    "        energy_quantization = model_config[\"variance_embedding\"][\"energy_quantization\"]\n",
    "        n_bins = model_config[\"variance_embedding\"][\"n_bins\"]\n",
    "        assert pitch_quantization in [\"linear\", \"log\"]\n",
    "        assert energy_quantization in [\"linear\", \"log\"]\n",
    "        with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \"stats.json\")) as f:\n",
    "            stats = json.load(f)\n",
    "            pitch_min, pitch_max = stats[\"pitch\"][:2]\n",
    "            energy_min, energy_max = stats[\"energy\"][:2]\n",
    "        if pitch_quantization == \"log\":\n",
    "            pitch_bins = np.exp(np.linspace(np.log(pitch_min), np.log(pitch_max), n_bins - 1))\n",
    "        else:\n",
    "            pitch_bins = np.linspace(pitch_min, pitch_max, n_bins - 1)\n",
    "        self.pitch_bins = ms.Parameter(ms.Tensor(pitch_bins, dtype=ms.float32), requires_grad=False)\n",
    "\n",
    "        if energy_quantization == \"log\":\n",
    "            energy_bins = np.exp(np.linspace(np.log(energy_min), np.log(energy_max), n_bins - 1))\n",
    "        else:\n",
    "            energy_bins = np.linspace(energy_min, energy_max, n_bins - 1)\n",
    "        self.energy_bins = ms.Parameter(ms.Tensor(energy_bins, dtype=ms.float32), requires_grad=False)\n",
    "        self.pitch_embedding = nn.Embedding(n_bins, model_config[\"transformer\"][\"encoder_hidden\"])\n",
    "        self.energy_embedding = nn.Embedding(n_bins, model_config[\"transformer\"][\"encoder_hidden\"])\n",
    "\n",
    "    def get_pitch_embedding(self, x, target, mask, control):\n",
    "        prediction = self.pitch_predictor(x, mask)\n",
    "        pitch_bins_list = self.pitch_bins.asnumpy().tolist()\n",
    "        pitch_bins_list = [float(bin_val) for bin_val in pitch_bins_list]\n",
    "        if target is not None:\n",
    "            bucket_idx = ops.bucketize(target, pitch_bins_list)\n",
    "            embedding = self.pitch_embedding(bucket_idx)\n",
    "        else:\n",
    "            prediction = prediction * control\n",
    "\n",
    "            bucket_idx = ops.bucketize(prediction, pitch_bins_list)\n",
    "            embedding = self.pitch_embedding(bucket_idx)\n",
    "        return prediction, embedding\n",
    "\n",
    "    def get_energy_embedding(self, x, target, mask, control):\n",
    "        prediction = self.energy_predictor(x, mask)\n",
    "        energy_bins_list = self.energy_bins.asnumpy().tolist()\n",
    "        energy_bins_list = [float(bin_val) for bin_val in energy_bins_list]\n",
    "        if target is not None:\n",
    "            bucket_idx = ops.bucketize(target, energy_bins_list)\n",
    "            embedding = self.energy_embedding(bucket_idx)\n",
    "        else:\n",
    "            prediction = prediction * control\n",
    "            bucket_idx = ops.bucketize(prediction, energy_bins_list)\n",
    "            embedding = self.energy_embedding(bucket_idx)\n",
    "        return prediction, embedding\n",
    "\n",
    "    def construct(\n",
    "        self, x, src_mask, mel_mask=None, max_len=None,\n",
    "        pitch_target=None, energy_target=None, duration_target=None,\n",
    "        p_control=1.0, e_control=1.0, d_control=1.0\n",
    "    ):\n",
    "        log_duration_prediction = self.duration_predictor(x, src_mask)\n",
    "        if self.pitch_feature_level == \"phoneme_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(x, pitch_target, src_mask, p_control)\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"phoneme_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(x, energy_target, src_mask, e_control)\n",
    "            x = x + energy_embedding\n",
    "        if duration_target is not None:\n",
    "            x, mel_len = self.length_regulator(x, duration_target, max_len)\n",
    "            duration_rounded = duration_target\n",
    "        else:\n",
    "            duration_rounded = ops.clamp((ops.round(ops.exp(log_duration_prediction) - 1) * d_control), min=0)\n",
    "            x, mel_len = self.length_regulator(x, duration_rounded, max_len)\n",
    "            mel_mask = get_mask_from_lengths(mel_len)\n",
    "        if self.pitch_feature_level == \"frame_level\":\n",
    "            pitch_prediction, pitch_embedding = self.get_pitch_embedding(x, pitch_target, mel_mask, p_control)\n",
    "            x = x + pitch_embedding\n",
    "        if self.energy_feature_level == \"frame_level\":\n",
    "            energy_prediction, energy_embedding = self.get_energy_embedding(x, energy_target, mel_mask, e_control)\n",
    "            x = x + energy_embedding\n",
    "        return x, pitch_prediction, energy_prediction, log_duration_prediction, duration_rounded, mel_len, mel_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9acdf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Cell):\n",
    "    def __init__(self, temperature):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.softmax = nn.Softmax(axis=2)\n",
    "\n",
    "    def construct(self, q, k, v, mask=None):\n",
    "        batch_size, len_q, d_k = q.shape\n",
    "        _, len_k, _ = k.shape\n",
    "        _, len_v, d_v = v.shape\n",
    "        k_trans = ops.transpose(k, (0, 2, 1))\n",
    "        attn = ops.matmul(q, k_trans)\n",
    "        attn = ops.clip_by_value(attn, clip_value_min=-8.0, clip_value_max=8.0)\n",
    "        attn = attn / self.temperature\n",
    "        if mask is not None:\n",
    "            attn = ops.masked_fill(attn, mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        output = ops.matmul(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab02ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Cell):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.w_qs = nn.Dense(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Dense(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Dense(d_model, n_head * d_v)\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm((d_model,))\n",
    "        self.fc = nn.Dense(n_head * d_v, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def construct(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        batch_size, len_q, d_model_q = q.shape\n",
    "        _, len_k, d_model_k = k.shape\n",
    "        _, len_v, d_model_v = v.shape\n",
    "\n",
    "\n",
    "        residual = q\n",
    "        residual = ops.cast(residual, ms.float32)\n",
    "        q = ops.cast(q, ms.float32)\n",
    "        k = ops.cast(k, ms.float32)\n",
    "        v = ops.cast(v, ms.float32)\n",
    "        q = self.w_qs(q)\n",
    "        k = self.w_ks(k)\n",
    "        v = self.w_vs(v)\n",
    "\n",
    "\n",
    "        q = q.reshape(batch_size, len_q, n_head, d_k)\n",
    "        k = k.reshape(batch_size, len_k, n_head, d_k)\n",
    "        v = v.reshape(batch_size, len_v, n_head, d_v)\n",
    "\n",
    "        transpose_axis = (0, 2, 1, 3)  # 转置轴：B→0, H→2, L→1, d→3\n",
    "\n",
    "        q = ops.transpose(q, transpose_axis)\n",
    "        k = ops.transpose(k, transpose_axis)\n",
    "        v = ops.transpose(v, transpose_axis)\n",
    "\n",
    "\n",
    "        q = q.reshape(-1, len_q, d_k)\n",
    "        k = k.reshape(-1, len_k, d_k)\n",
    "        v = v.reshape(-1, len_v, d_v)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_int = ops.cast(mask, ms.int32)\n",
    "            mask4_int = ops.repeat_interleave(mask_int, repeats=n_head, axis=0)\n",
    "            mask4 = ops.cast(mask4_int, ms.bool_)\n",
    "        else:\n",
    "            mask4 = None\n",
    "\n",
    "        output, attn = self.attention(q, k, v, mask=mask4)\n",
    "        output = output.reshape(batch_size, n_head, len_q, d_v)\n",
    "        output = ops.transpose(output, (0, 2, 1, 3))\n",
    "        output = output.reshape(batch_size, len_q, -1)\n",
    "\n",
    "        output = self.fc(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Cell):\n",
    "    def __init__(self, d_in, d_hid, kernel_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, (list, tuple)):\n",
    "            assert len(kernel_size) == 2, \"kernel_size 应为长度为2的序列，如 [9, 1]\"\n",
    "            ks1, ks2 = int(kernel_size[0]), int(kernel_size[1])\n",
    "        else:\n",
    "            ks1 = int(kernel_size)\n",
    "            ks2 = 1\n",
    "\n",
    "        pad1 = (ks1 - 1) // 2\n",
    "        pad2 = (ks2 - 1) // 2\n",
    "\n",
    "        self.w_1 = nn.Conv1d(\n",
    "            in_channels=d_in, out_channels=d_hid,\n",
    "            kernel_size=ks1, padding=pad1, pad_mode='pad',has_bias=True\n",
    "        )\n",
    "        self.w_2 = nn.Conv1d(\n",
    "            in_channels=d_hid, out_channels=d_in,\n",
    "            kernel_size=ks2, padding=pad2, pad_mode='pad',has_bias=True\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm((d_in,), epsilon=1e-4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def construct(self, x):\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)              # (B, T, C) -> (B, C, T)\n",
    "        x = self.w_2(ops.relu(self.w_1(x)))\n",
    "        x = x.transpose(1, 2)              # (B, C, T) -> (B, T, C)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x\n",
    "\n",
    "class FFTBlock(nn.Cell):\n",
    "    def __init__(self, d_model, n_head, d_k, d_v, d_inner, kernel_size, dropout=0.1):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, kernel_size, dropout=dropout)\n",
    "        self.norm_before_attn = nn.LayerNorm((d_model,), epsilon=1e-4)\n",
    "        self.norm_before_pos_ffn = nn.LayerNorm((d_model,), epsilon=1e-4)\n",
    "        self.norm_before_attn.gamma.set_data(ms.Tensor(np.ones(d_model) * 0.9, dtype=ms.float32))\n",
    "        self.norm_before_attn.beta.set_data(ms.Tensor(np.zeros(d_model), dtype=ms.float32))\n",
    "        self.norm_before_pos_ffn.gamma.set_data(ms.Tensor(np.ones(d_model) * 0.9, dtype=ms.float32))\n",
    "        self.norm_before_pos_ffn.beta.set_data(ms.Tensor(np.zeros(d_model), dtype=ms.float32))\n",
    "\n",
    "    def construct(self, enc_input, mask=None, slf_attn_mask=None):\n",
    "\n",
    "        enc_input_norm = self.norm_before_attn(enc_input)\n",
    "        enc_output, enc_slf_attn = self.slf_attn(enc_input_norm, enc_input_norm, enc_input_norm, mask=slf_attn_mask)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.expand_dims(2)\n",
    "            fill_value = ms.Tensor(0.0, dtype=enc_output.dtype)\n",
    "            enc_output = ops.masked_fill(enc_output, mask_expanded.astype(ms.bool_), fill_value)\n",
    "\n",
    "        enc_output = self.pos_ffn(self.norm_before_pos_ffn(enc_output))\n",
    "\n",
    "        if mask is not None:\n",
    "            enc_output = ops.masked_fill(enc_output, mask.expand_dims(-1).astype(ms.bool_), 0.0)\n",
    "        return enc_output, enc_slf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d6b35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNorm(nn.Cell):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None, dilation=1, bias=True):\n",
    "        super(ConvNorm, self).__init__()\n",
    "        if padding is None:\n",
    "            assert kernel_size % 2 == 1\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "            dilation=dilation, has_bias=bias,pad_mode='pad'\n",
    "        )\n",
    "\n",
    "    def construct(self, signal):\n",
    "        return self.conv(signal)\n",
    "\n",
    "\n",
    "class PostNet(nn.Cell):\n",
    "    def __init__(self, n_mel_channels=80, postnet_embedding_dim=512, postnet_kernel_size=5, postnet_n_convolutions=5):\n",
    "        super(PostNet, self).__init__()\n",
    "        self.convolutions = nn.CellList()\n",
    "        self.convolutions.append(nn.SequentialCell([\n",
    "            ConvNorm(n_mel_channels, postnet_embedding_dim, kernel_size=postnet_kernel_size,bias=True),\n",
    "            nn.BatchNorm1d(postnet_embedding_dim)\n",
    "        ]))\n",
    "        for _ in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(nn.SequentialCell([\n",
    "                ConvNorm(postnet_embedding_dim, postnet_embedding_dim, kernel_size=postnet_kernel_size,bias=True),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim)\n",
    "            ]))\n",
    "        self.convolutions.append(nn.SequentialCell([\n",
    "            ConvNorm(postnet_embedding_dim, n_mel_channels, kernel_size=postnet_kernel_size,bias=True),\n",
    "            nn.BatchNorm1d(n_mel_channels)\n",
    "        ]))\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = ops.tanh(self.convolutions[i](x))\n",
    "            x = nn.Dropout(p=0.5)(x) if self.training else x\n",
    "        x = self.convolutions[-1](x)\n",
    "        x = nn.Dropout(p=0.5)(x) if self.training else x\n",
    "        x = x.transpose(1, 2)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1a2f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        n_position = config[\"max_seq_len\"] + 1\n",
    "        n_src_vocab = len(symbols) + 1\n",
    "        d_word_vec = config[\"transformer\"][\"encoder_hidden\"]\n",
    "        n_layers = config[\"transformer\"][\"encoder_layer\"]\n",
    "        n_head = config[\"transformer\"][\"encoder_head\"]\n",
    "        d_k = d_v = config[\"transformer\"][\"encoder_hidden\"] // config[\"transformer\"][\"encoder_head\"]\n",
    "        d_model = config[\"transformer\"][\"encoder_hidden\"]\n",
    "        d_inner = config[\"transformer\"][\"conv_filter_size\"]\n",
    "        kernel_size = config[\"transformer\"][\"conv_kernel_size\"]\n",
    "        dropout = config[\"transformer\"][\"encoder_dropout\"]\n",
    "\n",
    "        self.max_seq_len = config[\"max_seq_len\"]\n",
    "        self.d_model = d_model\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=0)  # PAD=0\n",
    "        self.position_enc = ms.Parameter(\n",
    "            get_sinusoid_encoding_table(n_position, d_word_vec).expand_dims(0),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.layer_stack = nn.CellList([\n",
    "            FFTBlock(d_model, n_head, d_k, d_v, d_inner, kernel_size, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def construct(self, src_seq, mask, return_attns=False):\n",
    "        enc_slf_attn_list = []\n",
    "        batch_size, max_len = src_seq.shape[0], src_seq.shape[1]\n",
    "        slf_attn_mask = mask.expand_dims(1).broadcast_to((batch_size, max_len, max_len))\n",
    "        if not self.training and max_len > self.max_seq_len:\n",
    "            pos_enc = get_sinusoid_encoding_table(max_len, self.d_model).expand_dims(0)\n",
    "            pos_enc = pos_enc.broadcast_to((batch_size, max_len, self.d_model))\n",
    "        else:\n",
    "            pos_enc = self.position_enc[:, :max_len, :].broadcast_to((batch_size, max_len, self.d_model))\n",
    "\n",
    "        enc_output = self.src_word_emb(src_seq) + pos_enc\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, mask=mask, slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                enc_slf_attn_list.append(enc_slf_attn)\n",
    "\n",
    "        return enc_output if not return_attns else (enc_output, enc_slf_attn_list)\n",
    "\n",
    "\n",
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        n_position = config[\"max_seq_len\"] + 1\n",
    "        d_word_vec = config[\"transformer\"][\"decoder_hidden\"]\n",
    "        n_layers = config[\"transformer\"][\"decoder_layer\"]\n",
    "        n_head = config[\"transformer\"][\"decoder_head\"]\n",
    "        d_k = d_v = config[\"transformer\"][\"decoder_hidden\"] // config[\"transformer\"][\"decoder_head\"]\n",
    "        d_model = config[\"transformer\"][\"decoder_hidden\"]\n",
    "        d_inner = config[\"transformer\"][\"conv_filter_size\"]\n",
    "        kernel_size = config[\"transformer\"][\"conv_kernel_size\"]\n",
    "        dropout = config[\"transformer\"][\"decoder_dropout\"]\n",
    "\n",
    "        self.max_seq_len = config[\"max_seq_len\"]\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.position_enc = ms.Parameter(\n",
    "            get_sinusoid_encoding_table(n_position, d_word_vec).expand_dims(0),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.layer_stack = nn.CellList([\n",
    "            FFTBlock(d_model, n_head, d_k, d_v, d_inner, kernel_size, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def construct(self, enc_seq, mask, return_attns=False):\n",
    "        dec_slf_attn_list = []\n",
    "        batch_size, max_len = enc_seq.shape[0], enc_seq.shape[1]\n",
    "\n",
    "        isnan = ops.IsNan()\n",
    "        isinf = ops.IsInf()\n",
    "        any_op = ops.ReduceAny()\n",
    "\n",
    "\n",
    "        if not self.training and max_len > self.max_seq_len:\n",
    "            pos_enc = get_sinusoid_encoding_table(max_len, self.d_model).expand_dims(0)\n",
    "            pos_enc = pos_enc.broadcast_to((batch_size, max_len, self.d_model))\n",
    "            slf_attn_mask = mask.expand_dims(1).broadcast_to((batch_size, max_len, max_len))\n",
    "            dec_output = enc_seq + pos_enc\n",
    "        else:\n",
    "            max_len = min(max_len, self.max_seq_len)\n",
    "            enc_seq = enc_seq[:, :max_len, :]\n",
    "            mask = mask[:, :max_len]\n",
    "            pos_enc = self.position_enc[:, :max_len, :].broadcast_to((batch_size, max_len, self.d_model))\n",
    "            slf_attn_mask = mask.expand_dims(1).broadcast_to((batch_size, max_len, max_len))\n",
    "            dec_output = enc_seq + pos_enc\n",
    "\n",
    "\n",
    "        for layer_idx, dec_layer in enumerate(self.layer_stack):\n",
    "            dec_output, dec_slf_attn = dec_layer(dec_output, mask=mask, slf_attn_mask=slf_attn_mask)\n",
    "            if return_attns:\n",
    "                dec_slf_attn_list.append(dec_slf_attn)\n",
    "        return (dec_output, mask) if not return_attns else (dec_output, mask, dec_slf_attn_list)\n",
    "\n",
    "\n",
    "class FastSpeech2(nn.Cell):\n",
    "    def __init__(self, preprocess_config, model_config):\n",
    "        super(FastSpeech2, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        self.encoder = Encoder(model_config)\n",
    "        self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config)\n",
    "        self.decoder = Decoder(model_config)\n",
    "        self.mel_linear = nn.Dense(\n",
    "            model_config[\"transformer\"][\"decoder_hidden\"],\n",
    "            preprocess_config[\"preprocessing\"][\"mel\"][\"n_mel_channels\"]\n",
    "        )\n",
    "        self.postnet = PostNet()\n",
    "        self.speaker_emb = None\n",
    "        if model_config[\"multi_speaker\"]:\n",
    "            with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \"speakers.json\"), \"r\") as f:\n",
    "                n_speaker = len(json.load(f))\n",
    "            self.speaker_emb = nn.Embedding(n_speaker, model_config[\"transformer\"][\"encoder_hidden\"])\n",
    "\n",
    "\n",
    "    def construct(\n",
    "                self, speakers, texts, src_lens, max_src_len,\n",
    "                mels=None, mel_lens=None, max_mel_len=None,\n",
    "                p_targets=None, e_targets=None, d_targets=None,\n",
    "                p_control=1.0, e_control=1.0, d_control=1.0\n",
    "            ):\n",
    "        src_masks = get_mask_from_lengths(src_lens, max_src_len)\n",
    "        mel_masks = get_mask_from_lengths(mel_lens, max_mel_len) if mel_lens is not None else None\n",
    "        enc_output = self.encoder(texts, src_masks)\n",
    "\n",
    "        if self.speaker_emb is not None:\n",
    "            spk_emb = self.speaker_emb(speakers).expand_dims(1)\n",
    "            B, T, D = enc_output.shape\n",
    "            spk_emb = ops.broadcast_to(spk_emb, (B, T, spk_emb.shape[-1]))\n",
    "            enc_output = enc_output + spk_emb\n",
    "\n",
    "        (enc_output, p_predictions, e_predictions, log_d_predictions,\n",
    "        d_rounded, mel_lens, mel_masks) = self.variance_adaptor(\n",
    "            enc_output, src_masks, mel_masks, max_mel_len,\n",
    "            p_targets, e_targets, d_targets,\n",
    "            p_control, e_control, d_control\n",
    "        )\n",
    "        dec_output, mel_masks = self.decoder(enc_output, mel_masks)\n",
    "        mel_outputs = self.mel_linear(dec_output)\n",
    "        postnet_output = self.postnet(mel_outputs) + mel_outputs\n",
    "\n",
    "        return (\n",
    "            mel_outputs, postnet_output, p_predictions, e_predictions,\n",
    "            log_d_predictions, d_rounded, src_masks, mel_masks, src_lens, mel_lens\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58e70a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocoder(config, device):\n",
    "    name = config[\"vocoder\"][\"model\"]\n",
    "    speaker = config[\"vocoder\"][\"speaker\"]\n",
    "    if name == \"HiFi-GAN\":\n",
    "        with open(\"hifigan/config_SF.json\", \"r\") as f:\n",
    "            hifi_config = json.load(f)\n",
    "        vocoder = hifigan.Generator_SF(hifi_config)\n",
    "        ckpt_path = \"hifigan/generator_universal.ckpt\" if speaker == \"universal\" else \"hifigan/generator_LJSpeech.ckpt\"\n",
    "        param_dict = ms.load_checkpoint(ckpt_path)\n",
    "        param_not_load = ms.load_param_into_net(vocoder, param_dict,strict_load=True)\n",
    "        if param_not_load:\n",
    "            print(f\"警告：未加载的参数{param_not_load}，可能影响推理效果\")\n",
    "        vocoder.set_train(False)\n",
    "        vocoder.remove_weight_norm()\n",
    "        return vocoder\n",
    "    else:\n",
    "        raise ValueError(f\"不支持的声码器类型：{name}（当前仅支持MindSpore版HiFi-GAN）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15ad001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocoder_infer(mels, vocoder, model_config, preprocess_config, lengths=None):\n",
    "    name = model_config[\"vocoder\"][\"model\"]\n",
    "    if not isinstance(mels, ms.Tensor):\n",
    "        mels = ms.Tensor(mels, dtype=ms.float32)\n",
    "    vocoder.set_train(False)\n",
    "    if name == \"HiFi-GAN\":\n",
    "        wavs_ms = vocoder(mels).squeeze(1)  # 移除多余维度\n",
    "    max_wav_value = preprocess_config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n",
    "    wavs = (wavs_ms.asnumpy() * max_wav_value).astype(\"int16\")\n",
    "    wavs = [wav for wav in wavs]\n",
    "    if lengths is not None:\n",
    "        for i in range(len(wavs)):\n",
    "            length_scalar = int(lengths[i].asnumpy().item())\n",
    "            wavs[i] = wavs[i][:length_scalar]\n",
    "    return wavs\n",
    "\n",
    "def plot_mel(data, stats, titles):\n",
    "    fig, axes = plt.subplots(len(data), 1, squeeze=False)\n",
    "    titles = titles if titles else [None]*len(data)\n",
    "    pitch_min, pitch_max, pitch_mean, pitch_std, energy_min, energy_max = stats\n",
    "    pitch_min = pitch_min * pitch_std + pitch_mean\n",
    "    pitch_max = pitch_max * pitch_std + pitch_mean\n",
    "    def add_axis(fig, old_ax):\n",
    "        ax = fig.add_axes(old_ax.get_position(), anchor=\"W\")\n",
    "        ax.set_facecolor(\"None\")\n",
    "        return ax\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        mel, pitch, energy = data[i]\n",
    "        pitch = pitch * pitch_std + pitch_mean\n",
    "        axes[i][0].imshow(mel, origin=\"lower\")\n",
    "        axes[i][0].set_aspect(2.5, adjustable=\"box\")\n",
    "        axes[i][0].set_ylim(0, mel.shape[0])\n",
    "        axes[i][0].set_title(titles[i], fontsize=\"medium\")\n",
    "        axes[i][0].tick_params(labelsize=\"x-small\", left=False, labelleft=False)\n",
    "        axes[i][0].set_anchor(\"W\")\n",
    "        ax1 = add_axis(fig, axes[i][0])\n",
    "        ax1.plot(pitch, color=\"tomato\")\n",
    "        ax1.set_xlim(0, mel.shape[1])\n",
    "        ax1.set_ylim(0, pitch_max)\n",
    "        ax1.set_ylabel(\"F0\", color=\"tomato\")\n",
    "        ax1.tick_params(labelsize=\"x-small\", colors=\"tomato\", bottom=False, labelbottom=False)\n",
    "        ax2 = add_axis(fig, axes[i][0])\n",
    "        ax2.plot(energy, color=\"darkviolet\")\n",
    "        ax2.set_xlim(0, mel.shape[1])\n",
    "        ax2.set_ylim(energy_min, energy_max)\n",
    "        ax2.set_ylabel(\"Energy\", color=\"darkviolet\")\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        ax2.tick_params(labelsize=\"x-small\", colors=\"darkviolet\", bottom=False, labelbottom=False, left=False, labelleft=False, right=True, labelright=True)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def synth_samples(targets, predictions, vocoder, model_config, preprocess_config, path):\n",
    "    basenames = [f\"sample_{i}\" for i in range(len(predictions[0]))]  # 样本名称\n",
    "    os.makedirs(path, exist_ok=True)  # 创建保存目录\n",
    "\n",
    "    with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \"stats.json\")) as f:\n",
    "        stats = json.load(f)\n",
    "        stats = stats[\"pitch\"] + stats[\"energy\"][:2]  # [p_min, p_max, p_mean, p_std, e_min, e_max]\n",
    "\n",
    "    for i in range(len(predictions[0])):\n",
    "        mel_len = int(predictions[9][i].asnumpy())  # Mel谱实际长度\n",
    "        mel_pred = predictions[1][i, :mel_len].transpose(0, 1).asnumpy()  # (mel_dim, seq_len)\n",
    "        src_len = int(predictions[8][i].asnumpy())  # 文本序列长度\n",
    "        duration = predictions[5][i, :src_len].asnumpy()  # 时长预测结果\n",
    "\n",
    "        if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n",
    "            pitch = predictions[2][i, :src_len].asnumpy()\n",
    "            pitch = expand(pitch, duration)\n",
    "        else:\n",
    "            pitch = predictions[2][i, :mel_len].asnumpy()\n",
    "\n",
    "        if preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"] == \"phoneme_level\":\n",
    "            energy = predictions[3][i, :src_len].asnumpy()\n",
    "            energy = expand(energy, duration)\n",
    "        else:\n",
    "            energy = predictions[3][i, :mel_len].asnumpy()\n",
    "\n",
    "        fig = plot_mel([(mel_pred, pitch, energy)], stats, [\"Synthetized Spectrogram\"])\n",
    "        plt.savefig(os.path.join(path, f\"{basenames[i]}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    mel_predictions = predictions[1].transpose(1, 2)  # (batch, mel_dim, seq_len)\n",
    "    hop_length = preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "    lengths = predictions[9] * hop_length  # 音频实际长度（采样点）\n",
    "    wav_predictions = vocoder_infer(mel_predictions, vocoder, model_config, preprocess_config, lengths=lengths)\n",
    "\n",
    "    sampling_rate = preprocess_config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
    "    for wav, basename in zip(wav_predictions, basenames):\n",
    "        wavfile.write(os.path.join(path, f\"{basename}.wav\"), sampling_rate, wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f70111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fastspeech2_model(preprocess_config, model_config, ckpt_path):\n",
    "    model = FastSpeech2(preprocess_config, model_config)\n",
    "    param_dict = ms.load_checkpoint(ckpt_path)\n",
    "    encoder_layer_num = model_config[\"transformer\"][\"encoder_layer\"]\n",
    "    for layer_idx in range(encoder_layer_num):\n",
    "        w1_name = f\"encoder.layer_stack.{layer_idx}.pos_ffn.w_1.weight\"\n",
    "        if w1_name in param_dict:\n",
    "            old_w1 = param_dict[w1_name].asnumpy()\n",
    "            new_w1 = old_w1.reshape(1024, 256, 1, 9)\n",
    "            param_dict[w1_name] = ms.Parameter(ms.Tensor(new_w1, dtype=ms.float32), requires_grad=False)\n",
    "        w2_name = f\"encoder.layer_stack.{layer_idx}.pos_ffn.w_2.weight\"\n",
    "        if w2_name in param_dict:\n",
    "            old_w2 = param_dict[w2_name].asnumpy()\n",
    "            new_w2 = old_w2.reshape(256, 1024, 1, 1)\n",
    "            param_dict[w2_name] = ms.Parameter(ms.Tensor(new_w2, dtype=ms.float32), requires_grad=False)\n",
    "    \n",
    "    decoder_layer_num = model_config[\"transformer\"][\"decoder_layer\"]\n",
    "    for layer_idx in range(decoder_layer_num):\n",
    "        w1_name = f\"decoder.layer_stack.{layer_idx}.pos_ffn.w_1.weight\"\n",
    "        if w1_name in param_dict:\n",
    "            old_w1 = param_dict[w1_name].asnumpy()\n",
    "            new_w1 = old_w1.reshape(1024, 256, 1, 9)\n",
    "            param_dict[w1_name] = ms.Parameter(ms.Tensor(new_w1, dtype=ms.float32), requires_grad=False)\n",
    "        w2_name = f\"decoder.layer_stack.{layer_idx}.pos_ffn.w_2.weight\"\n",
    "        if w2_name in param_dict:\n",
    "            old_w2 = param_dict[w2_name].asnumpy()\n",
    "            new_w2 = old_w2.reshape(256, 1024, 1, 1)\n",
    "            param_dict[w2_name] = ms.Parameter(ms.Tensor(new_w2, dtype=ms.float32), requires_grad=False)\n",
    "    \n",
    "    predictors = [\"duration_predictor\", \"pitch_predictor\", \"energy_predictor\"]\n",
    "    conv_layers = [\"conv1d_1\", \"conv1d_2\"]\n",
    "    for predictor in predictors:\n",
    "        for conv_layer in conv_layers:\n",
    "            param_name = f\"variance_adaptor.{predictor}.conv_layer.{conv_layer}.conv.weight\"\n",
    "            if param_name in param_dict:\n",
    "                old_weight = param_dict[param_name].asnumpy()\n",
    "                new_weight = old_weight.reshape(256, 256, 1, 3)\n",
    "                param_dict[param_name] = ms.Parameter(ms.Tensor(new_weight, dtype=ms.float32), requires_grad=False)\n",
    "    \n",
    "    postnet_conv_num = 5 \n",
    "    for conv_idx in range(postnet_conv_num):\n",
    "        param_name = f\"postnet.convolutions.{conv_idx}.0.conv.weight\"\n",
    "        if param_name in param_dict:\n",
    "            old_weight = param_dict[param_name].asnumpy()\n",
    "            new_weight = old_weight.reshape(*old_weight.shape[:2], 1, old_weight.shape[2])\n",
    "            param_dict[param_name] = ms.Parameter(ms.Tensor(new_weight, dtype=ms.float32), requires_grad=False)\n",
    "    \n",
    "    # -------------------------- 5. 加载参数 --------------------------\n",
    "    param_not_load = ms.load_param_into_net(model, param_dict)\n",
    "    model.set_train(False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_vocoder(model_config):\n",
    "    return get_vocoder(model_config, device)\n",
    "\n",
    "\n",
    "def infer_single_sentence():\n",
    "    preprocess_config = preprocess_config_infer\n",
    "    model_config = model_config_infer\n",
    "    control_params = infer_control_params\n",
    "    input_data = infer_input\n",
    "    ckpt_path = \"./output/ckpt/AISHELL3/600000.ckpt\"\n",
    "    model = load_fastspeech2_model(preprocess_config, model_config, ckpt_path)\n",
    "    vocoder = load_vocoder(model_config)\n",
    "    text_sequence = preprocess_mandarin(input_data[\"text\"], preprocess_config)\n",
    "    text_sequence = ms.Tensor(text_sequence, dtype=ms.int64) \n",
    "    speakers = ms.Tensor([input_data[\"speaker_id\"]], dtype=ms.int64)  # 说话人ID\n",
    "    texts = text_sequence.expand_dims(0)  # (1, seq_len)：批量大小=1\n",
    "    src_lens = ms.Tensor([text_sequence.shape[0]], dtype=ms.int64)  # 文本长度\n",
    "    max_src_len = int(text_sequence.shape[0])  # ✅ 纯 Python int\n",
    "    context.set_context(mode=context.PYNATIVE_MODE, device_target='CPU')  # 或 GPU，根据需要选择\n",
    "    output = model(\n",
    "        speakers=speakers,\n",
    "        texts=texts,\n",
    "        src_lens=src_lens,\n",
    "        max_src_len=max_src_len,\n",
    "        p_control=control_params[\"p_control\"],  # 音高控制\n",
    "        e_control=control_params[\"e_control\"],  # 能量控制\n",
    "        d_control=control_params[\"d_control\"]   # 语速控制\n",
    "    )\n",
    "    save_path = \"./output/result/AISHELL3\"\n",
    "    batch = (\n",
    "        [input_data[\"text\"][:100]],  # 文本摘要\n",
    "        [input_data[\"text\"]],        # 原始文本\n",
    "        speakers.asnumpy(),          # 说话人ID（numpy）\n",
    "        texts.asnumpy(),             # 文本序列（numpy）\n",
    "        src_lens.asnumpy(),          # 文本长度（numpy）\n",
    "        max_src_len              # 最大文本长度（numpy）\n",
    "    )\n",
    "    synth_samples(batch, output, vocoder, model_config, preprocess_config, save_path)\n",
    "    print(f\"语音合成完成！结果保存至：{save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dca7c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_config_infer = {\n",
    "    \"path\": {\n",
    "        \"lexicon_path\": \"lexicon/pinyin-lexicon-r.txt\",  # 音素词典路径\n",
    "        \"preprocessed_path\": \"./preprocessed_data/AISHELL3\",  # 预处理数据路径（含stats.json）\n",
    "        \"speakers_json_path\": \"./preprocessed_data/AISHELL3/speakers.json\"  # 说话人列表\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"text\": {\n",
    "            \"language\": \"zh\",  # 中文\n",
    "            \"text_cleaners\": []  # 文本清理规则（与训练一致）\n",
    "        },\n",
    "        \"audio\": {\n",
    "            \"sampling_rate\": 22050,  # 采样率（与声码器一致）\n",
    "            \"max_wav_value\": 32768.0  # 音频最大值（int16范围）\n",
    "        },\n",
    "        \"stft\": {\n",
    "            \"filter_length\": 1024,\n",
    "            \"hop_length\": 256,\n",
    "            \"win_length\": 1024\n",
    "        },\n",
    "        \"mel\": {\n",
    "            \"n_mel_channels\": 80,  # Mel谱通道数（与模型输出一致）\n",
    "            \"mel_fmin\": 0,\n",
    "            \"mel_fmax\": 8000  # HiFi-GAN常用配置\n",
    "        },\n",
    "        \"pitch\": {\n",
    "            \"feature\": \"phoneme_level\",  # 音素级音高（与训练一致）\n",
    "            \"normalization\": True\n",
    "        },\n",
    "        \"energy\": {\n",
    "            \"feature\": \"phoneme_level\",  # 音素级能量（与训练一致）\n",
    "            \"normalization\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_config_infer = {\n",
    "    \"transformer\": {\n",
    "        \"encoder_layer\": 4,\n",
    "        \"encoder_head\": 2,\n",
    "        \"encoder_hidden\": 256,\n",
    "        \"decoder_layer\": 6,\n",
    "        \"decoder_head\": 2,\n",
    "        \"decoder_hidden\": 256,\n",
    "        \"conv_filter_size\": 1024,\n",
    "        \"conv_kernel_size\": [9, 1],\n",
    "        \"encoder_dropout\": 0.2,\n",
    "        \"decoder_dropout\": 0.3\n",
    "    },\n",
    "    \"variance_predictor\": {\n",
    "        \"filter_size\": 256,\n",
    "        \"kernel_size\": 3,\n",
    "        \"dropout\": 0.5\n",
    "    },\n",
    "    \"variance_embedding\": {\n",
    "        \"pitch_quantization\": \"linear\",\n",
    "        \"energy_quantization\": \"linear\",\n",
    "        \"n_bins\": 256\n",
    "    },\n",
    "    \"multi_speaker\": True,  # 多说话人模型\n",
    "    \"max_seq_len\": 1000,    # 最大文本长度\n",
    "    \"vocoder\": {\n",
    "        \"model\": \"HiFi-GAN\",  # 声码器类型\n",
    "        \"speaker\": \"universal\"  # 通用声码器\n",
    "    }\n",
    "}\n",
    "\n",
    "infer_control_params = {\n",
    "    \"p_control\": 1.0,\n",
    "    \"e_control\": 1.0, \n",
    "    \"d_control\": 1.0  \n",
    "}\n",
    "\n",
    "infer_input = {\n",
    "    \"text\": \"李白，字太白，号青莲居士，唐代伟大的浪漫主义诗人，被后人誉为“诗仙”。他的诗歌以豪放飘逸、想象丰富著称，代表作有《将进酒》《静夜思》《早发白帝城》等，深受人们喜爱。\",\n",
    "    \"speaker_id\": 0,\n",
    "    \"max_src_len\": 1000\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c73a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(29708:25668,MainProcess):2025-11-02-23:07:38.694.000 [mindspore\\train\\serialization.py:1761] For 'load_param_into_net', 40 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(29708:25668,MainProcess):2025-11-02-23:07:38.694.000 [mindspore\\train\\serialization.py:1765] ['encoder.layer_stack.0.norm_before_attn.gamma', 'encoder.layer_stack.0.norm_before_attn.beta', 'encoder.layer_stack.0.norm_before_pos_ffn.gamma', 'encoder.layer_stack.0.norm_before_pos_ffn.beta', 'encoder.layer_stack.1.norm_before_attn.gamma', 'encoder.layer_stack.1.norm_before_attn.beta', 'encoder.layer_stack.1.norm_before_pos_ffn.gamma', 'encoder.layer_stack.1.norm_before_pos_ffn.beta', 'encoder.layer_stack.2.norm_before_attn.gamma', 'encoder.layer_stack.2.norm_before_attn.beta', 'encoder.layer_stack.2.norm_before_pos_ffn.gamma', 'encoder.layer_stack.2.norm_before_pos_ffn.beta', 'encoder.layer_stack.3.norm_before_attn.gamma', 'encoder.layer_stack.3.norm_before_attn.beta', 'encoder.layer_stack.3.norm_before_pos_ffn.gamma', 'encoder.layer_stack.3.norm_before_pos_ffn.beta', 'decoder.layer_stack.0.norm_before_attn.gamma', 'decoder.layer_stack.0.norm_before_attn.beta', 'decoder.layer_stack.0.norm_before_pos_ffn.gamma', 'decoder.layer_stack.0.norm_before_pos_ffn.beta', 'decoder.layer_stack.1.norm_before_attn.gamma', 'decoder.layer_stack.1.norm_before_attn.beta', 'decoder.layer_stack.1.norm_before_pos_ffn.gamma', 'decoder.layer_stack.1.norm_before_pos_ffn.beta', 'decoder.layer_stack.2.norm_before_attn.gamma', 'decoder.layer_stack.2.norm_before_attn.beta', 'decoder.layer_stack.2.norm_before_pos_ffn.gamma', 'decoder.layer_stack.2.norm_before_pos_ffn.beta', 'decoder.layer_stack.3.norm_before_attn.gamma', 'decoder.layer_stack.3.norm_before_attn.beta', 'decoder.layer_stack.3.norm_before_pos_ffn.gamma', 'decoder.layer_stack.3.norm_before_pos_ffn.beta', 'decoder.layer_stack.4.norm_before_attn.gamma', 'decoder.layer_stack.4.norm_before_attn.beta', 'decoder.layer_stack.4.norm_before_pos_ffn.gamma', 'decoder.layer_stack.4.norm_before_pos_ffn.beta', 'decoder.layer_stack.5.norm_before_attn.gamma', 'decoder.layer_stack.5.norm_before_attn.beta', 'decoder.layer_stack.5.norm_before_pos_ffn.gamma', 'decoder.layer_stack.5.norm_before_pos_ffn.beta'] are not loaded.\n",
      "[WARNING] ME(29708:25668,MainProcess):2025-11-02-23:07:38.914.000 [mindspore\\context.py:1401] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告：未加载的参数([], [])，可能影响推理效果\n",
      "原始文本: 李白，字太白，号青莲居士，唐代伟大的浪漫主义诗人，被后人誉为“诗仙”。他的诗歌以豪放飘逸、想象丰富著称，代表作有《将进酒》《静夜思》《早发白帝城》等，深受人们喜爱。\n",
      "音素序列: {l i3 b ai2 sp z ii4 t ai4 b ai2 sp h ao4 q ing1 l ian2 j v1 sh iii4 sp t ang2 d ai4 w uei3 d a4 d e5 l ang4 m an4 zh u3 y i4 sh iii1 r en2 sp b ei4 h ou4 r en2 y v4 w uei2 sp sh iii1 x ian1 sp t a1 d e5 sh iii1 g e1 y i3 h ao2 f ang4 p iao1 y i4 sp x iang3 x iang4 f eng1 f u4 zh u4 ch eng1 sp d ai4 b iao3 z uo4 y iou3 sp q iang1 j in4 j iou3 sp j ing4 y ie4 s ii1 sp z ao3 f a1 b ai2 d i4 ch eng2 sp d eng3 sp sh en1 sh ou4 r en2 m en5 x i3 ai4 sp}\n",
      "语音合成完成！结果保存至：./output/result/AISHELL3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    infer_single_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
